{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load proper agent\n",
    "from environments import build_blue_agent, build_red_agent, sample\n",
    "\n",
    "tolerance = 3 # number of batches without improvement before ending training\n",
    "generations = 100\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(opponent=True)\n",
    "red = build_red_agent()\n",
    "\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(1, generations+1):\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    red.restore(\"./policies/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    while True:\n",
    "        b += 1\n",
    "        result = red.train()\n",
    "        red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "        vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF_loss: {vf_loss:0.2f}\")\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/red_competitive_pool/competitive_red_{g}\")   \n",
    "                path_file = open(f\"./policies/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(\"./policies/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(\"./policies/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    while True:\n",
    "        b += 1\n",
    "        result = blue.train()\n",
    "        blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "        vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\") \n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path) \n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(\"./policies/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print()\n",
    "    \n",
    "    print(f'-------- Sample Game for Generation {g} --------')\n",
    "    sample(red, blue, verbose=True, show_policy=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "    path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "    red_restore_path = path_file.read()\n",
    "    path_file.close()\n",
    "    red.restore(red_restore_path)\n",
    "\n",
    "    # iterate through blue opponents\n",
    "    for b in range(generations,0,-1):\n",
    "        path_file = open(f\"./policies/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "        blue_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        blue.restore(blue_restore_path)\n",
    "\n",
    "        score = sample(red, blue, games=50)\n",
    "        if score < r_min[r]:\n",
    "            r_min[r] = score\n",
    "            r_min_op[r] = b\n",
    "    \n",
    "    print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.')\n",
    "    if r_min[r] > r_best_score:\n",
    "        r_best_score = r_min[r]\n",
    "        r_best_id = r\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "\n",
    "path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(\"./policies/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34568a2c-4a47-4cf9-afa8-ff1a5228f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "    path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "    red_restore_path = path_file.read()\n",
    "    path_file.close()\n",
    "    red.restore(red_restore_path)\n",
    "\n",
    "    # iterate through blue opponents\n",
    "    for b in range(generations,0,-1):\n",
    "        path_file = open(f\"./policies/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "        blue_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        blue.restore(blue_restore_path)\n",
    "\n",
    "        score = sample(red, blue, games=50)\n",
    "        if score < r_min[r]:\n",
    "            r_min[r] = score\n",
    "            r_min_op[r] = b\n",
    "    \n",
    "    print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.')\n",
    "    if r_min[r] > r_best_score:\n",
    "        r_best_score = r_min[r]\n",
    "        r_best_id = r\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "\n",
    "path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(\"./policies/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bed55-31ba-4dc9-ab8e-99281c262d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "    path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "    red_restore_path = path_file.read()\n",
    "    path_file.close()\n",
    "    red.restore(red_restore_path)\n",
    "\n",
    "    # iterate through blue opponents\n",
    "    for b in range(generations,0,-1):\n",
    "        path_file = open(f\"./policies/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "        blue_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        blue.restore(blue_restore_path)\n",
    "\n",
    "        score = sample(red, blue, games=50)\n",
    "        if score < r_min[r]:\n",
    "            r_min[r] = score\n",
    "            r_min_op[r] = b\n",
    "    \n",
    "    print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.')\n",
    "    if r_min[r] > r_best_score:\n",
    "        r_best_score = r_min[r]\n",
    "        r_best_id = r\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "\n",
    "path_file = open(f\"./policies/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(\"./policies/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5240f2-7007-430a-bf17-6a1ee6011cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r_min' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m r_exp \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# exploitability of each Red Policy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mr_min\u001b[49m[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m      3\u001b[0m     r_exp\u001b[38;5;241m.\u001b[39mappend(r_best_score\u001b[38;5;241m-\u001b[39mr)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(r_exp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r_min' is not defined"
     ]
    }
   ],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db29ab3-10f7-4fdd-8d9a-23769016ac2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'red_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Scores During Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m id_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mid\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(\u001b[43mred_scores\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m      3\u001b[0m data_plot \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m:id_plot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Score\u001b[39m\u001b[38;5;124m\"\u001b[39m:red_scores, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlue Scores\u001b[39m\u001b[38;5;124m\"\u001b[39m:blue_scores})\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'red_scores' is not defined"
     ]
    }
   ],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
