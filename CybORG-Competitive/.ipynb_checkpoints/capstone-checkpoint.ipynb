{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  274360\n",
      "mini_batch_size:  27436\n",
      "rollout_fragment_length:  68590\n"
     ]
    }
   ],
   "source": [
    "from environments import build_blue_agent, build_red_agent, build_cardiff_agent, sample, get_timesteps, get_algorithm_select\n",
    "\n",
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5189ab-336e-4eef-bc6f-35c92b2137fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_cardiff_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### Testing code for building a cardiff blue agent ( to be moved down to the correct cell later below!)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m blue_cardiff \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_cardiff_agent\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_cardiff_agent' is not defined"
     ]
    }
   ],
   "source": [
    "### Testing code for building a cardiff blue agent ( to be moved down to the correct cell later below!)\n",
    "blue_cardiff = build_cardiff_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {},
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting blue impala config\n",
      "./policies/impala/30/blue_opponent_pool/opponent_blue_0\\checkpoint_000000\n",
      "Pass 1\n",
      "selecting red impala config\n",
      "./policies/impala/30/red_competitive_pool/competitive_red_0\\checkpoint_000000\n",
      "Pass 2\n",
      "\n",
      "+--------------------------------+\n",
      "| Red Competitive Training Start |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "| Generation 1 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 3.29    Entropy: 3.64    VF Loss: 0.59    Execution time: 00:00:02\n",
      "Batch 2 -- Red Score: 2.96    Entropy: 0.01    VF Loss: 0.08    Execution time: 00:00:10\n",
      "Batch 3 -- Red Score: 2.81    Entropy: 0.00    VF Loss: 0.25    Execution time: 00:00:10\n",
      "Batch 4 -- Red Score: 2.78    Entropy: 0.00    VF Loss: 0.22    Execution time: 00:00:10\n",
      "Batch 5 -- Red Score: 2.61    Entropy: 0.00    VF Loss: 0.16    Execution time: 00:00:10\n",
      "Batch time: 00:00:42\n",
      "./policies/impala/30/red_competitive_pool/competitive_red_1\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.08    Entropy: 4.80    VF Loss: 0.60    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 1.78    Entropy: 4.03    VF Loss: 0.00    Execution time: 00:00:10\n",
      "Batch 3 -- Blue Score: 1.53    Entropy: 4.57    VF Loss: 0.00    Execution time: 00:00:10\n",
      "Batch 4 -- Blue Score: 0.57    Entropy: 4.57    VF Loss: 0.00    Execution time: 00:00:10\n",
      "Batch 5 -- Blue Score: -0.00    Entropy: 1.40    VF Loss: 0.00    Execution time: 00:00:09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    115\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 116\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mblue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    118\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:349\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    348\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 349\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:673\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m     (\n\u001b[0;32m    666\u001b[0m         results,\n\u001b[0;32m    667\u001b[0m         train_iter_ctx,\n\u001b[0;32m    668\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2584\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2582\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step()\n\u001b[0;32m   2583\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2584\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exec_impl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:786\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:1115\u001b[0m, in \u001b[0;36mLocalIterator.union.<locals>.build_union\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pull):\n\u001b[1;32m-> 1115\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:786\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\concurrency_ops.py:139\u001b[0m, in \u001b[0;36mDequeue.<locals>.base_iterator\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m check():\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[43minput_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\learner_thread.py\", line 74, in run\n",
      "    self.step()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\multi_gpu_learner_thread.py\", line 143, in step\n",
      "    buffer_idx, released = self.ready_tower_stacks_buffer.get()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\minibatch_buffer.py\", line 48, in get\n",
      "    self.buffers[self.idx] = self.inqueue.get(timeout=self.timeout)\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\queue.py\", line 178, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\learner_thread.py\", line 74, in run\n",
      "    self.step()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\multi_gpu_learner_thread.py\", line 143, in step\n",
      "    buffer_idx, released = self.ready_tower_stacks_buffer.get()\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\minibatch_buffer.py\", line 48, in get\n",
      "    self.buffers[self.idx] = self.inqueue.get(timeout=self.timeout)\n",
      "  File \"C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\queue.py\", line 178, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "\n",
    "# Select which generation we want to start training from\n",
    "# If we want to train from scratch set to 1 (default value is 1)\n",
    "# If we want to train from the latest generation, set to latest generation in the competitive policy pool\n",
    "start_from_generation = 1\n",
    "\n",
    "# Load rewards if training from a checkpointed generation\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "# Total number of generations create for red agent\n",
    "generations = 25\n",
    "\n",
    "# Error checking: Update either the 'starting_from_generation' or 'total_generations' parameter\n",
    "if(start_from_generation > generations):\n",
    "    raise ValueError(\"Starting generation and Total Generation incompatible\")\n",
    "\n",
    "# Number of batches without improvement before ending training\n",
    "tolerance = 3\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(fresh=True, opponent=True)\n",
    "print(\"Pass 1\")\n",
    "red = build_red_agent(fresh=True)\n",
    "print(\"Pass 2\")\n",
    "\n",
    "# Convert float string into a float\n",
    "# Useful if loading rewards from certain generation\n",
    "blue_scores = [float(item) for item in blue_scores]\n",
    "red_scores = [float(item) for item in red_scores]\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(start_from_generation, generations+1):\n",
    "\n",
    "    # Time how long each generation takes\n",
    "    g_time = time.time()\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    red.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = red.train()\n",
    "        # print(result)\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        # Algorithm specific score retrieval\n",
    "        red_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            red_score = result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = blue.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        \n",
    "        # Score retrieval based on algorithm\n",
    "        blue_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            blue_score = -result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print(\"Total time: \",time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_time)))\n",
    "    print()\n",
    "    \n",
    "    # print(f'-------- Sample Game for Generation {g} --------')\n",
    "    # sample(red, blue, verbose=True, show_policy=True)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample games (default is 50)\n",
    "sample_games = 1\n",
    "\n",
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "g_start = time.time()\n",
    "\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "\n",
    "    # If the generation exists\n",
    "    if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\")):\n",
    "        start = time.time()\n",
    "        path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "        red_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        red.restore(red_restore_path)\n",
    "    \n",
    "        # iterate through blue opponents\n",
    "        for b in range(generations,0,-1):\n",
    "\n",
    "            # If the generation exists\n",
    "            if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\")):\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "                blue_restore_path = path_file.read()\n",
    "                path_file.close()\n",
    "                blue.restore(blue_restore_path)\n",
    "                score = sample(red, blue, games=sample_games)\n",
    "                if score < r_min[r]:\n",
    "                    r_min[r] = score\n",
    "                    r_min_op[r] = b\n",
    "                    \n",
    "        print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.', end=\"\\t\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start)))\n",
    "        if r_min[r] > r_best_score:\n",
    "            r_best_score = r_min[r]\n",
    "            r_best_id = r\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "print(\"Total Execution Time: \", time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_start)))\n",
    "\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb8796-bb25-4d45-b98a-980f22a65e79",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores (First 5 points dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45227c17-9bf1-449c-9375-6dc56941b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Training Score\":red_scores[5:], \"Blue Scores\":blue_scores[5:]})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Min Expected Score\":r_min[6:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Exploitability\":r_exp[5:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef761fd-1a08-4a7d-8f7e-ee4ad00b07ca",
   "metadata": {},
   "source": [
    "#### Cardiff Blue Agent vs Competitive Red PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ee342-a1b6-412c-96a6-d8600b532001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Agent - Restore the best checkpointed path\n",
    "# Blu Agent - Restore Cardiff blue agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5275b6a-7f76-4c31-ad51-debb5e3184fe",
   "metadata": {},
   "source": [
    "#### Cardiff Blue Agent & Dedicated Red PPO Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666bc0-21c3-4c41-995f-5f9cc4eb63f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
