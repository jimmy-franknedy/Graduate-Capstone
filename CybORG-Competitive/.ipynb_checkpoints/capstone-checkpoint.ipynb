{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import build_blue_agent, build_red_agent, sample, get_timesteps, get_algorithm_select\n",
    "\n",
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {},
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting blue impala config\n",
      "./policies/impala/30/blue_opponent_pool/opponent_blue_0\\checkpoint_000000\n",
      "Pass 1\n",
      "selecting red impala config\n",
      "./policies/impala/30/red_competitive_pool/competitive_red_0\\checkpoint_000000\n",
      "Pass 2\n",
      "\n",
      "+--------------------------------+\n",
      "| Red Competitive Training Start |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "| Generation 1 |\n",
      "+--------------+\n",
      "\n",
      "{'episode_reward_max': 6.0, 'episode_reward_min': 1.0, 'episode_reward_mean': 3.54375, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 16, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [2.0, 2.8000000000000007, 2.0, 4.299999999999999, 3.0, 6.0, 2.0, 2.0, 3.0, 6.0, 6.0, 4.6, 1.0, 3.0, 5.0, 4.000000000000001], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.7047335231114948, 'mean_inference_ms': 1.2967184422508118, 'mean_action_processing_ms': 0.10392306342957511, 'mean_env_wait_ms': 5.45640786488851, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'timesteps_total': 300, 'timesteps_this_iter': 100, 'agent_timesteps_total': 300, 'timers': {'sample_time_ms': 359.148, 'sample_throughput': 278.436}, 'info': {'num_steps_sampled': 300, 'num_agent_steps_sampled': 300, 'num_steps_trained_this_iter': 100, 'num_steps_trained': 100, 'learner_queue': {'size_count': 6, 'size_mean': 0.0, 'size_std': 0.0, 'size_quantiles': [0.0, 0.0, 0.0, 0.0, 0.0]}, 'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'policy_loss': 3.1725852, 'entropy': 3.6343238, 'entropy_coeff': 0.0010000000474974513, 'var_gnorm': 32.004677, 'vf_loss': 0.9027892, 'vf_explained_var': 0.0024229884, 'model': {}, 'grad_gnorm': 40.0}, 'custom_metrics': {}, 'num_agent_steps_trained': 100.0}}, 'timing_breakdown': {'learner_grad_time_ms': 76.076, 'learner_load_time_ms': 0.0, 'learner_load_wait_time_ms': 118.162, 'learner_dequeue_time_ms': 729.054}}, 'num_recreated_workers': 0, 'done': False, 'episodes_total': 16, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '8c28cbc95f714d0c853b00639106a1da', 'date': '2024-04-19_10-56-35', 'timestamp': 1713549395, 'time_this_iter_s': 1.1881134510040283, 'time_total_s': 1.1881134510040283, 'pid': 17344, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219925F5370>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219913509D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 1.1881134510040283, 'timesteps_since_restore': 100, 'iterations_since_restore': 1, 'warmup_time': 9.801866054534912}\n",
      "Batch 1 -- Red Score: 3.54    Entropy: 3.63    VF Loss: 0.90    Execution time: 00:00:01\n",
      "{'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.862285714285714, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 175, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3.0, 1.5000000000000004, 3.0, 1.0, 3.0, 5.0, 8.0, 5.0, 3.0, 4.0, 2.0, 4.0, 2.0, 3.0, 4.0, 2.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 5.0, 2.0, 4.0, 0.0, 1.0, 2.0, 3.0, 5.0, 1.0, 0.0, 0.0, 3.0, 3.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 0.0, 3.0, 4.0, 3.0, 1.0, 1.0, 3.0, 6.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 3.0, 6.0, 1.0, 1.0, 5.0, 2.0, 4.0, 1.0, 3.0, 3.1, 4.0, 3.5000000000000004, 2.0, 5.0, 6.0, 3.0, 3.0, 1.0, 3.0, 1.0, 4.0, 1.4000000000000004, 5.0, 5.600000000000001, 4.0, 5.399999999999999, 4.0, 3.0, 2.0, 4.0, 6.0, 3.0, 4.0, 3.0, 1.0, 3.0, 5.0, 2.0, 4.0, 2.3000000000000003, 1.0, 3.0, 4.0, 3.0, 3.0, 3.0, 1.0, 4.000000000000002, 6.0, 1.0, 1.0, 2.0, 5.1, 1.0, 5.0, 1.0, 2.0, 3.0, 3.0, 0.0, 2.0, 5.0, 2.0, 4.0, 4.0, 3.0, 3.0, 4.0, 1.0, 3.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0, 3.0, 4.0, 2.0, 6.0, 2.0, 4.0, 1.0, 2.0, 5.0, 8.0, 3.0, 3.0, 5.0, 4.0, 4.0, 1.0, 4.0, 2.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 4.0, 3.0, 1.0, 2.0, 1.0, 3.0, 2.0, 3.0, 3.0, 2.0, 5.0, 2.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.3571273545181444, 'mean_inference_ms': 0.9559909228525498, 'mean_action_processing_ms': 0.10573978643606141, 'mean_env_wait_ms': 4.279401446717699, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'timesteps_total': 5575, 'timesteps_this_iter': 100, 'agent_timesteps_total': 5575, 'timers': {'sample_time_ms': 177.904, 'sample_throughput': 562.1}, 'info': {'num_steps_sampled': 5575, 'num_agent_steps_sampled': 5575, 'num_steps_trained_this_iter': 100, 'num_steps_trained': 119500, 'num_weight_broadcasts': 52, 'learner_queue': {'size_count': 1215, 'size_mean': 0.0, 'size_std': 0.0, 'size_quantiles': [0.0, 0.0, 0.0, 0.0, 0.0]}, 'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'policy_loss': 0.094944365, 'entropy': 1.5070131, 'entropy_coeff': 0.0010000000474974513, 'var_gnorm': 34.224808, 'vf_loss': 0.044603586, 'vf_explained_var': 0.46203995, 'model': {}, 'grad_gnorm': 16.081964}, 'custom_metrics': {}, 'num_agent_steps_trained': 100.0}}, 'timing_breakdown': {'learner_grad_time_ms': 4.97, 'learner_load_time_ms': 0.0, 'learner_load_wait_time_ms': 0.0, 'learner_dequeue_time_ms': 2508.114}}, 'num_recreated_workers': 0, 'done': False, 'episodes_total': 191, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '8c28cbc95f714d0c853b00639106a1da', 'date': '2024-04-19_10-56-45', 'timestamp': 1713549405, 'time_this_iter_s': 10.268532514572144, 'time_total_s': 11.456645965576172, 'pid': 17344, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219925F5370>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219913509D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 11.456645965576172, 'timesteps_since_restore': 200, 'iterations_since_restore': 2, 'warmup_time': 9.801866054534912}\n",
      "Batch 2 -- Red Score: 2.86    Entropy: 1.51    VF Loss: 0.04    Execution time: 00:00:10\n",
      "{'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.4347826086956523, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 161, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 3.0, 0.0, 1.0, 2.0, 5.0, 2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0, 2.0, 1.0, 4.0, 3.0, 3.0, 2.0, 5.0, 6.0, 1.0, 4.0, 2.0, 4.0, 4.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 7.0, 5.0, 1.0, 2.0, 3.0, 4.0, 2.0, 2.0, 2.0, 1.0, 1.0, 4.0, 2.0, 3.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 4.0, 3.0, 4.0, 2.0, 3.0, 0.0, 3.0, 5.0, 4.0, 1.0, 2.0, 3.0, 3.0, 2.0, 5.0, 1.0, 4.0, 0.0, 2.0, 4.0, 5.0, 3.0, 3.0, 2.0, 3.0, 4.0, 5.0, 3.0, 1.0, 3.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 4.0, 3.0, 2.0, 0.0, 1.0, 1.0, 5.0, 2.0, 3.0, 2.0, 4.0, 4.0, 0.0, 2.0, 3.0, 2.0, 3.0, 6.0, 1.0, 2.0, 1.0, 2.0, 6.0, 4.0, 3.0, 2.0, 5.0, 1.0, 2.0, 2.0, 0.0, 3.0, 0.0, 3.0, 1.0, 1.0, 8.0, 1.0, 2.0, 4.0, 2.0, 2.0, 4.0, 4.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 0.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 4.0, 6.0, 4.0, 4.0, 4.0, 0.0, 2.0, 2.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.43420978728141, 'mean_inference_ms': 0.9709125589559, 'mean_action_processing_ms': 0.10980905453752302, 'mean_env_wait_ms': 4.2859349154299995, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'timesteps_total': 10400, 'timesteps_this_iter': 100, 'agent_timesteps_total': 10400, 'timers': {'sample_time_ms': 207.107, 'sample_throughput': 482.843}, 'info': {'num_steps_sampled': 10400, 'num_agent_steps_sampled': 10400, 'num_steps_trained_this_iter': 100, 'num_steps_trained': 265600, 'num_weight_broadcasts': 101, 'learner_queue': {'size_count': 2685, 'size_mean': 0.0, 'size_std': 0.0, 'size_quantiles': [0.0, 0.0, 0.0, 0.0, 0.0]}, 'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'policy_loss': 8.727923e-05, 'entropy': 0.004087129, 'entropy_coeff': 0.0010000000474974513, 'var_gnorm': 36.040386, 'vf_loss': 0.2478202, 'vf_explained_var': 0.9527342, 'model': {}, 'grad_gnorm': 1.6788894}, 'custom_metrics': {}, 'num_agent_steps_trained': 100.0}}, 'timing_breakdown': {'learner_grad_time_ms': 4.729, 'learner_load_time_ms': 0.0, 'learner_load_wait_time_ms': 0.0, 'learner_dequeue_time_ms': 2807.675}}, 'num_recreated_workers': 0, 'done': False, 'episodes_total': 352, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': '8c28cbc95f714d0c853b00639106a1da', 'date': '2024-04-19_10-56-55', 'timestamp': 1713549415, 'time_this_iter_s': 9.796715497970581, 'time_total_s': 21.253361463546753, 'pid': 17344, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219925F5370>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219913509D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 21.253361463546753, 'timesteps_since_restore': 300, 'iterations_since_restore': 3, 'warmup_time': 9.801866054534912}\n",
      "Batch 3 -- Red Score: 2.43    Entropy: 0.00    VF Loss: 0.25    Execution time: 00:00:09\n",
      "{'episode_reward_max': 7.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.6407185628742513, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 167, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [2.0, 4.0, 4.0, 1.0, 4.0, 2.0, 3.0, 0.0, 3.0, 1.0, 1.0, 7.0, 3.0, 4.0, 2.0, 1.0, 3.0, 5.0, 2.0, 0.0, 3.0, 3.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 4.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 1.0, 5.0, 1.0, 2.0, 1.0, 3.0, 7.0, 3.0, 7.0, 2.0, 2.0, 2.0, 1.0, 5.0, 4.0, 2.0, 1.0, 2.0, 1.0, 4.0, 4.0, 6.0, 3.0, 2.0, 4.0, 4.0, 2.0, 5.0, 1.0, 3.0, 3.0, 5.0, 2.0, 4.0, 0.0, 6.0, 3.0, 3.0, 0.0, 4.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 3.0, 4.0, 2.0, 7.0, 5.0, 3.0, 1.0, 2.0, 0.0, 4.0, 2.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 3.0, 2.0, 6.0, 4.0, 4.0, 3.0, 4.0, 6.0, 3.0, 1.0, 3.0, 2.0, 3.0, 1.0, 5.0, 2.0, 4.0, 5.0, 3.0, 5.0, 2.0, 4.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 1.0, 3.0, 3.0, 0.0, 3.0, 1.0, 2.0, 3.0, 5.0, 1.0, 3.0, 2.0, 0.0, 6.0, 4.0, 4.0, 4.0, 2.0, 5.0, 1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 3.0, 4.0, 2.0, 3.0, 2.0, 3.0, 5.0, 2.0, 2.0, 4.0, 1.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.3866750886905246, 'mean_inference_ms': 0.9457813471018851, 'mean_action_processing_ms': 0.1009914243352019, 'mean_env_wait_ms': 4.343078953727705, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'timesteps_total': 15450, 'timesteps_this_iter': 100, 'agent_timesteps_total': 15450, 'timers': {'sample_time_ms': 204.279, 'sample_throughput': 489.526}, 'info': {'num_steps_sampled': 15450, 'num_agent_steps_sampled': 15450, 'num_steps_trained_this_iter': 100, 'num_steps_trained': 416600, 'num_weight_broadcasts': 151, 'learner_queue': {'size_count': 4185, 'size_mean': 0.0, 'size_std': 0.0, 'size_quantiles': [0.0, 0.0, 0.0, 0.0, 0.0]}, 'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'policy_loss': -3.3496792e-07, 'entropy': 0.00017453328, 'entropy_coeff': 0.0010000000474974513, 'var_gnorm': 36.53036, 'vf_loss': 0.101263866, 'vf_explained_var': 0.6039187, 'model': {}, 'grad_gnorm': 0.5825822}, 'custom_metrics': {}, 'num_agent_steps_trained': 100.0}}, 'timing_breakdown': {'learner_grad_time_ms': 3.674, 'learner_load_time_ms': 0.0, 'learner_load_wait_time_ms': 0.94, 'learner_dequeue_time_ms': 2928.063}}, 'num_recreated_workers': 0, 'done': False, 'episodes_total': 519, 'training_iteration': 4, 'trial_id': 'default', 'experiment_id': '8c28cbc95f714d0c853b00639106a1da', 'date': '2024-04-19_10-57-05', 'timestamp': 1713549425, 'time_this_iter_s': 9.947746515274048, 'time_total_s': 31.2011079788208, 'pid': 17344, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219925F5370>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219913509D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 31.2011079788208, 'timesteps_since_restore': 400, 'iterations_since_restore': 4, 'warmup_time': 9.801866054534912}\n",
      "Batch 4 -- Red Score: 2.64    Entropy: 0.00    VF Loss: 0.10    Execution time: 00:00:09\n",
      "{'episode_reward_max': 8.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.8588235294117648, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 170, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [2.0, 6.0, 4.0, 3.0, 4.0, 2.0, 1.0, 5.0, 1.0, 1.0, 5.0, 1.0, 2.0, 4.0, 8.0, 3.0, 1.0, 2.0, 2.0, 2.0, 5.0, 2.0, 6.0, 4.0, 4.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 2.0, 5.0, 3.0, 2.0, 5.0, 2.0, 4.0, 3.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 2.0, 3.0, 1.0, 0.0, 4.0, 3.0, 4.0, 2.0, 1.0, 4.0, 3.0, 3.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 3.0, 4.0, 2.0, 5.0, 6.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 5.0, 7.0, 1.0, 1.0, 8.0, 3.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 1.0, 4.0, 0.0, 1.0, 2.0, 1.0, 4.0, 5.0, 1.0, 4.0, 5.0, 3.0, 6.0, 0.0, 1.0, 3.0, 1.0, 5.0, 3.0, 3.0, 1.0, 2.0, 4.0, 5.0, 2.0, 4.0, 2.0, 2.0, 1.0, 3.0, 3.0, 1.0, 5.0, 7.0, 6.0, 4.0, 6.0, 3.0, 3.0, 1.0, 2.0, 4.0, 1.0, 2.0, 2.0, 0.0, 2.0, 6.0, 3.0, 2.0, 2.0, 3.0, 1.0, 3.0, 2.0, 3.0, 7.0, 7.0, 2.0, 2.0, 1.0, 3.0, 4.0, 2.0, 1.0, 2.0, 1.0, 5.0, 2.0, 4.0, 4.0, 1.0, 1.0, 5.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.4415248032927748, 'mean_inference_ms': 0.9416486130093019, 'mean_action_processing_ms': 0.09816694402287413, 'mean_env_wait_ms': 4.335409292824068, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'timesteps_total': 20500, 'timesteps_this_iter': 100, 'agent_timesteps_total': 20500, 'timers': {'sample_time_ms': 204.312, 'sample_throughput': 489.447}, 'info': {'num_steps_sampled': 20500, 'num_agent_steps_sampled': 20500, 'num_steps_trained_this_iter': 100, 'num_steps_trained': 568600, 'num_weight_broadcasts': 202, 'learner_queue': {'size_count': 5715, 'size_mean': 0.0, 'size_std': 0.0, 'size_quantiles': [0.0, 0.0, 0.0, 0.0, 0.0]}, 'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'policy_loss': 4.7993785e-07, 'entropy': 0.0002055246, 'entropy_coeff': 0.0010000000474974513, 'var_gnorm': 37.439064, 'vf_loss': 0.24391732, 'vf_explained_var': 0.43344498, 'model': {}, 'grad_gnorm': 0.66992855}, 'custom_metrics': {}, 'num_agent_steps_trained': 100.0}}, 'timing_breakdown': {'learner_grad_time_ms': 4.771, 'learner_load_time_ms': 0.0, 'learner_load_wait_time_ms': 0.0, 'learner_dequeue_time_ms': 3205.733}}, 'num_recreated_workers': 0, 'done': False, 'episodes_total': 689, 'training_iteration': 5, 'trial_id': 'default', 'experiment_id': '8c28cbc95f714d0c853b00639106a1da', 'date': '2024-04-19_10-57-15', 'timestamp': 1713549435, 'time_this_iter_s': 10.321852445602417, 'time_total_s': 41.52296042442322, 'pid': 17344, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 100, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219925F5370>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': False, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 30, 'replay_proportion': 0.0, 'replay_ratio': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 4, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_sampler_worker': 2, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.01, 'entropy_coeff': 0.001, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219913509D0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 41.52296042442322, 'timesteps_since_restore': 500, 'iterations_since_restore': 5, 'warmup_time': 9.801866054534912}\n",
      "Batch 5 -- Red Score: 2.86    Entropy: 0.00    VF Loss: 0.24    Execution time: 00:00:10\n",
      "Batch time: 00:00:41\n",
      "./policies/impala/30/red_competitive_pool/competitive_red_1\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.38    Entropy: 4.80    VF Loss: 0.12    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 1.35    Entropy: 4.63    VF Loss: 0.00    Execution time: 00:00:09\n",
      "Batch 3 -- Blue Score: 0.23    Entropy: 4.70    VF Loss: 0.00    Execution time: 00:00:10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    115\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 116\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mblue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    118\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:349\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    348\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 349\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:673\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m     (\n\u001b[0;32m    666\u001b[0m         results,\n\u001b[0;32m    667\u001b[0m         train_iter_ctx,\n\u001b[0;32m    668\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2584\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2582\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step()\n\u001b[0;32m   2583\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2584\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exec_impl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:786\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:876\u001b[0m, in \u001b[0;36mLocalIterator.filter.<locals>.apply_filter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_filter\u001b[39m(it):\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady) \u001b[38;5;129;01mor\u001b[39;00m fn(item):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:1115\u001b[0m, in \u001b[0;36mLocalIterator.union.<locals>.build_union\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pull):\n\u001b[1;32m-> 1115\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:786\u001b[0m, in \u001b[0;36mLocalIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_once()\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilt_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:814\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_foreach\u001b[39m(it):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, _NextValueNotReady):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\util\\iter.py:822\u001b[0m, in \u001b[0;36mLocalIterator.for_each.<locals>.apply_foreach\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_context():\n\u001b[1;32m--> 822\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m result\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, _NextValueNotReady):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py:434\u001b[0m, in \u001b[0;36mBroadcastUpdateLearnerWeights.__call__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_since_broadcast \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_since_broadcast \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbroadcast_interval\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner_thread\u001b[38;5;241m.\u001b[39mweights_updated\n\u001b[0;32m    433\u001b[0m ):\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_since_broadcast \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner_thread\u001b[38;5;241m.\u001b[39mweights_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\worker.py:2345\u001b[0m, in \u001b[0;36mput\u001b[1;34m(value, _owner)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profiling\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray.put\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m         object_ref \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialize_owner_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ObjectStoreFullError:\n\u001b[0;32m   2347\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPut failed since the value was either too large or the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore was full of pinned objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2350\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\worker.py:620\u001b[0m, in \u001b[0;36mWorker.put_object\u001b[1;34m(self, value, object_ref, owner_address)\u001b[0m\n\u001b[0;32m    612\u001b[0m serialized_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_serialization_context()\u001b[38;5;241m.\u001b[39mserialize(value)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# This *must* be the first place that we construct this python\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# ObjectRef because an entry with 0 local references is created when\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# the object is Put() in the core worker, expecting that this python\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# reference will be created. If another reference is created and\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# removed before this one, it will corrupt the state in the\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# reference counter.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mObjectRef(\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_serialized_object_and_increment_local_ref\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialized_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mowner_address\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;66;03m# The initial local reference is already acquired internally.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m     skip_adding_local_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    625\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "\n",
    "# Select which generation we want to start training from\n",
    "# If we want to train from scratch set to 1 (default value is 1)\n",
    "# If we want to train from the latest generation, set to latest generation in the competitive policy pool\n",
    "start_from_generation = 1\n",
    "\n",
    "# Load rewards if training from a checkpointed generation\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "# Total number of generations create for red agent\n",
    "generations = 25\n",
    "\n",
    "# Error checking: Update either the 'starting_from_generation' or 'total_generations' parameter\n",
    "if(start_from_generation > generations):\n",
    "    raise ValueError(\"Starting generation and Total Generation incompatible\")\n",
    "\n",
    "# Number of batches without improvement before ending training\n",
    "tolerance = 3\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(fresh=True, opponent=True)\n",
    "print(\"Pass 1\")\n",
    "red = build_red_agent(fresh=True)\n",
    "print(\"Pass 2\")\n",
    "\n",
    "# Convert float string into a float\n",
    "# Useful if loading rewards from certain generation\n",
    "blue_scores = [float(item) for item in blue_scores]\n",
    "red_scores = [float(item) for item in red_scores]\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(start_from_generation, generations+1):\n",
    "\n",
    "    # Time how long each generation takes\n",
    "    g_time = time.time()\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    red.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = red.train()\n",
    "        # print(result)\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        # Algorithm specific score retrieval\n",
    "        red_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            red_score = result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = blue.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        \n",
    "        # Score retrieval based on algorithm\n",
    "        blue_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            blue_score = -result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print(\"Total time: \",time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_time)))\n",
    "    print()\n",
    "    \n",
    "    # print(f'-------- Sample Game for Generation {g} --------')\n",
    "    # sample(red, blue, verbose=True, show_policy=True)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample games (default is 50)\n",
    "sample_games = 1\n",
    "\n",
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "g_start = time.time()\n",
    "\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "\n",
    "    # If the generation exists\n",
    "    if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\")):\n",
    "        start = time.time()\n",
    "        path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "        red_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        red.restore(red_restore_path)\n",
    "    \n",
    "        # iterate through blue opponents\n",
    "        for b in range(generations,0,-1):\n",
    "\n",
    "            # If the generation exists\n",
    "            if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\")):\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "                blue_restore_path = path_file.read()\n",
    "                path_file.close()\n",
    "                blue.restore(blue_restore_path)\n",
    "                score = sample(red, blue, games=sample_games)\n",
    "                if score < r_min[r]:\n",
    "                    r_min[r] = score\n",
    "                    r_min_op[r] = b\n",
    "                    \n",
    "        print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.', end=\"\\t\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start)))\n",
    "        if r_min[r] > r_best_score:\n",
    "            r_best_score = r_min[r]\n",
    "            r_best_id = r\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "print(\"Total Execution Time: \", time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_start)))\n",
    "\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb8796-bb25-4d45-b98a-980f22a65e79",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores (First 5 points dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45227c17-9bf1-449c-9375-6dc56941b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Training Score\":red_scores[5:], \"Blue Scores\":blue_scores[5:]})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Min Expected Score\":r_min[6:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Exploitability\":r_exp[5:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
