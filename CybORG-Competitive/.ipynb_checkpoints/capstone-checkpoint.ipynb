{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO agent environment\n",
    "from environments import build_blue_agent, build_red_agent, sample, get_timesteps, get_algorithm_select\n",
    "\n",
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {},
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting blue ppo config\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_0\\checkpoint_000000\n",
      "Pass 1\n",
      "selecting red ppo config\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_0\\checkpoint_000000\n",
      "Pass 2\n",
      "\n",
      "+--------------------------------+\n",
      "| Red Competitive Training Start |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "| Generation 1 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: nan    Entropy: 3.62    VF Loss: 1.59    Execution time: 00:00:02\n",
      "Batch 2 -- Red Score: 1.75    Entropy: 3.62    VF Loss: 0.86    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.00    Entropy: 3.61    VF Loss: 0.33    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.50    Entropy: 3.60    VF Loss: 0.98    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.71    Entropy: 3.59    VF Loss: 0.53    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.60    Entropy: 3.58    VF Loss: 0.19    Execution time: 00:00:01\n",
      "Batch 7 -- Red Score: 2.60    Entropy: 3.58    VF Loss: 0.16    Execution time: 00:00:01\n",
      "Batch 8 -- Red Score: 2.46    Entropy: 3.58    VF Loss: 0.25    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_1\\checkpoint_000005\n",
      "\n",
      "Batch 1 -- Blue Score: nan    Entropy: 4.79    VF Loss: 0.51    Execution time: 00:00:02\n",
      "Batch 2 -- Blue Score: 2.25    Entropy: 4.79    VF Loss: 0.58    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.75    Entropy: 4.79    VF Loss: 0.39    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.42    Entropy: 4.79    VF Loss: 0.45    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.32    Entropy: 4.78    VF Loss: 0.19    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_1\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25']\n",
      "Red Scores so far ['2.71']\n",
      "Total time:  00:00:23\n",
      "\n",
      "+--------------+\n",
      "| Generation 2 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.64    Entropy: 3.62    VF Loss: 0.95    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.79    Entropy: 3.62    VF Loss: 0.87    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.76    Entropy: 3.61    VF Loss: 0.25    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.80    Entropy: 3.60    VF Loss: 0.82    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.80    Entropy: 3.60    VF Loss: 0.77    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.84    Entropy: 3.60    VF Loss: 0.28    Execution time: 00:00:01\n",
      "Batch 7 -- Red Score: 2.85    Entropy: 3.59    VF Loss: 0.25    Execution time: 00:00:01\n",
      "Batch 8 -- Red Score: 2.81    Entropy: 3.58    VF Loss: 0.33    Execution time: 00:00:01\n",
      "Batch 9 -- Red Score: 2.82    Entropy: 3.56    VF Loss: 0.84    Execution time: 00:00:01\n",
      "Batch 10 -- Red Score: 2.81    Entropy: 3.55    VF Loss: 0.98    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_2\\checkpoint_000007\n",
      "\n",
      "Batch 1 -- Blue Score: 2.32    Entropy: 4.79    VF Loss: 1.17    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.32    Entropy: 4.79    VF Loss: 0.80    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.45    Entropy: 4.79    VF Loss: 0.43    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.57    Entropy: 4.79    VF Loss: 0.23    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.47    Entropy: 4.78    VF Loss: 0.38    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_2\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32']\n",
      "Red Scores so far ['2.71', '2.85']\n",
      "Total time:  00:00:24\n",
      "\n",
      "+--------------+\n",
      "| Generation 3 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.81    Entropy: 3.62    VF Loss: 1.12    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.74    Entropy: 3.61    VF Loss: 1.34    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.82    Entropy: 3.61    VF Loss: 0.71    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.80    Entropy: 3.61    VF Loss: 0.31    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.77    Entropy: 3.59    VF Loss: 0.24    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.74    Entropy: 3.58    VF Loss: 0.51    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_3\\checkpoint_000003\n",
      "\n",
      "Batch 1 -- Blue Score: 2.41    Entropy: 4.79    VF Loss: 0.92    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.50    Entropy: 4.79    VF Loss: 0.56    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.50    Entropy: 4.79    VF Loss: 0.44    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.52    Entropy: 4.79    VF Loss: 0.36    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.54    Entropy: 4.79    VF Loss: 0.15    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_3\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50']\n",
      "Red Scores so far ['2.71', '2.85', '2.82']\n",
      "Total time:  00:00:17\n",
      "\n",
      "+--------------+\n",
      "| Generation 4 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.74    Entropy: 3.62    VF Loss: 0.21    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.65    Entropy: 3.62    VF Loss: 0.49    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.64    Entropy: 3.61    VF Loss: 0.16    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.64    Entropy: 3.60    VF Loss: 0.57    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.63    Entropy: 3.60    VF Loss: 0.24    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_4\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.53    Entropy: 4.79    VF Loss: 0.71    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.54    Entropy: 4.79    VF Loss: 0.60    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.57    Entropy: 4.79    VF Loss: 1.31    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.57    Entropy: 4.79    VF Loss: 1.12    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.60    Entropy: 4.78    VF Loss: 0.29    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_4\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65']\n",
      "Total time:  00:00:15\n",
      "\n",
      "+--------------+\n",
      "| Generation 5 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.56    Entropy: 3.62    VF Loss: 0.12    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.56    Entropy: 3.62    VF Loss: 0.70    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.70    Entropy: 3.61    VF Loss: 0.46    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.71    Entropy: 3.60    VF Loss: 0.41    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.67    Entropy: 3.59    VF Loss: 0.43    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.60    Entropy: 3.58    VF Loss: 0.18    Execution time: 00:00:01\n",
      "Batch 7 -- Red Score: 2.62    Entropy: 3.59    VF Loss: 0.63    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_5\\checkpoint_000004\n",
      "\n",
      "Batch 1 -- Blue Score: 2.55    Entropy: 4.79    VF Loss: 0.34    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.52    Entropy: 4.79    VF Loss: 1.02    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.58    Entropy: 4.79    VF Loss: 1.27    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.57    Entropy: 4.79    VF Loss: 0.25    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.57    Entropy: 4.78    VF Loss: 0.64    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_5\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71']\n",
      "Total time:  00:00:18\n",
      "\n",
      "+--------------+\n",
      "| Generation 6 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.62    Entropy: 3.62    VF Loss: 1.04    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.65    Entropy: 3.62    VF Loss: 0.17    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.56    Entropy: 3.61    VF Loss: 0.27    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.49    Entropy: 3.60    VF Loss: 0.52    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.52    Entropy: 3.60    VF Loss: 0.48    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_6\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.55    Entropy: 4.79    VF Loss: 0.19    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.50    Entropy: 4.80    VF Loss: 0.39    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.55    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.54    Entropy: 4.79    VF Loss: 0.69    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.23    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_6\\checkpoint_000002\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52', '2.50']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71', '2.65']\n",
      "Total time:  00:00:16\n",
      "\n",
      "+--------------+\n",
      "| Generation 7 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.51    Entropy: 3.62    VF Loss: 0.82    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.51    Entropy: 3.62    VF Loss: 0.80    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.50    Entropy: 3.61    VF Loss: 0.52    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.46    Entropy: 3.60    VF Loss: 0.56    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.50    Entropy: 3.59    VF Loss: 0.66    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_7\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 1.66    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.56    Entropy: 4.79    VF Loss: 0.48    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.52    Entropy: 4.79    VF Loss: 0.52    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.55    Entropy: 4.79    VF Loss: 0.58    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.55    Entropy: 4.79    VF Loss: 0.46    Execution time: 00:00:01\n",
      "Batch 6 -- Blue Score: 2.60    Entropy: 4.78    VF Loss: 1.91    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_7\\checkpoint_000003\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52', '2.50', '2.52']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71', '2.65', '2.51']\n",
      "Total time:  00:00:17\n",
      "\n",
      "+--------------+\n",
      "| Generation 8 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.49    Entropy: 3.62    VF Loss: 0.65    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.46    Entropy: 3.62    VF Loss: 0.95    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.46    Entropy: 3.61    VF Loss: 0.34    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.49    Entropy: 3.60    VF Loss: 1.38    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.48    Entropy: 3.59    VF Loss: 0.29    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.48    Entropy: 3.58    VF Loss: 0.32    Execution time: 00:00:01\n",
      "Batch 7 -- Red Score: 2.48    Entropy: 3.58    VF Loss: 0.93    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_8\\checkpoint_000004\n",
      "\n",
      "Batch 1 -- Blue Score: 2.60    Entropy: 4.79    VF Loss: 0.74    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.57    Entropy: 4.79    VF Loss: 0.25    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.54    Entropy: 4.78    VF Loss: 0.59    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.61    Entropy: 4.78    VF Loss: 0.36    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.56    Entropy: 4.78    VF Loss: 0.31    Execution time: 00:00:01\n",
      "Batch 6 -- Blue Score: 2.49    Entropy: 4.78    VF Loss: 0.27    Execution time: 00:00:01\n",
      "Batch 7 -- Blue Score: 2.49    Entropy: 4.77    VF Loss: 0.77    Execution time: 00:00:01\n",
      "Batch 8 -- Blue Score: 2.55    Entropy: 4.76    VF Loss: 0.22    Execution time: 00:00:01\n",
      "Batch 9 -- Blue Score: 2.52    Entropy: 4.76    VF Loss: 0.47    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_8\\checkpoint_000006\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52', '2.50', '2.52', '2.49']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71', '2.65', '2.51', '2.49']\n",
      "Total time:  00:00:25\n",
      "\n",
      "+--------------+\n",
      "| Generation 9 |\n",
      "+--------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.53    Entropy: 3.62    VF Loss: 1.17    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.53    Entropy: 3.62    VF Loss: 2.34    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.62    Entropy: 3.61    VF Loss: 0.72    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.63    Entropy: 3.59    VF Loss: 0.60    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.65    Entropy: 3.59    VF Loss: 0.39    Execution time: 00:00:01\n",
      "Batch 6 -- Red Score: 2.68    Entropy: 3.58    VF Loss: 0.93    Execution time: 00:00:01\n",
      "Batch 7 -- Red Score: 2.75    Entropy: 3.58    VF Loss: 0.69    Execution time: 00:00:02\n",
      "Batch 8 -- Red Score: 2.75    Entropy: 3.57    VF Loss: 0.69    Execution time: 00:00:01\n",
      "Batch 9 -- Red Score: 2.72    Entropy: 3.56    VF Loss: 0.50    Execution time: 00:00:01\n",
      "Batch 10 -- Red Score: 2.77    Entropy: 3.55    VF Loss: 0.40    Execution time: 00:00:01\n",
      "Batch 11 -- Red Score: 2.78    Entropy: 3.55    VF Loss: 0.67    Execution time: 00:00:01\n",
      "Batch 12 -- Red Score: 2.79    Entropy: 3.54    VF Loss: 0.35    Execution time: 00:00:01\n",
      "Batch 13 -- Red Score: 2.76    Entropy: 3.55    VF Loss: 0.66    Execution time: 00:00:01\n",
      "Batch 14 -- Red Score: 2.76    Entropy: 3.53    VF Loss: 0.64    Execution time: 00:00:01\n",
      "Batch 15 -- Red Score: 2.76    Entropy: 3.53    VF Loss: 0.61    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_9\\checkpoint_000012\n",
      "\n",
      "Batch 1 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.42    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.27    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.47    Entropy: 4.79    VF Loss: 0.21    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.47    Entropy: 4.79    VF Loss: 0.78    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.49    Entropy: 4.78    VF Loss: 0.19    Execution time: 00:00:01\n",
      "Batch 6 -- Blue Score: 2.54    Entropy: 4.78    VF Loss: 0.66    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_9\\checkpoint_000003\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52', '2.50', '2.52', '2.49', '2.47']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71', '2.65', '2.51', '2.49', '2.79']\n",
      "Total time:  00:00:33\n",
      "\n",
      "+---------------+\n",
      "| Generation 10 |\n",
      "+---------------+\n",
      "\n",
      "Batch 1 -- Red Score: 2.78    Entropy: 3.62    VF Loss: 0.33    Execution time: 00:00:01\n",
      "Batch 2 -- Red Score: 2.78    Entropy: 3.62    VF Loss: 0.20    Execution time: 00:00:01\n",
      "Batch 3 -- Red Score: 2.71    Entropy: 3.61    VF Loss: 0.09    Execution time: 00:00:01\n",
      "Batch 4 -- Red Score: 2.72    Entropy: 3.60    VF Loss: 1.12    Execution time: 00:00:01\n",
      "Batch 5 -- Red Score: 2.72    Entropy: 3.59    VF Loss: 0.50    Execution time: 00:00:01\n",
      "./policies/ppo/30/red_competitive_pool/competitive_red_10\\checkpoint_000002\n",
      "\n",
      "Batch 1 -- Blue Score: 2.59    Entropy: 4.79    VF Loss: 0.51    Execution time: 00:00:01\n",
      "Batch 2 -- Blue Score: 2.50    Entropy: 4.79    VF Loss: 0.32    Execution time: 00:00:01\n",
      "Batch 3 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.38    Execution time: 00:00:01\n",
      "Batch 4 -- Blue Score: 2.51    Entropy: 4.79    VF Loss: 0.15    Execution time: 00:00:01\n",
      "Batch 5 -- Blue Score: 2.44    Entropy: 4.79    VF Loss: 0.17    Execution time: 00:00:01\n",
      "Batch 6 -- Blue Score: 2.45    Entropy: 4.78    VF Loss: 0.72    Execution time: 00:00:01\n",
      "Batch 7 -- Blue Score: 2.43    Entropy: 4.78    VF Loss: 0.18    Execution time: 00:00:01\n",
      "Batch 8 -- Blue Score: 2.44    Entropy: 4.77    VF Loss: 0.55    Execution time: 00:00:01\n",
      "Batch 9 -- Blue Score: 2.45    Entropy: 4.77    VF Loss: 0.40    Execution time: 00:00:01\n",
      "Batch 10 -- Blue Score: 2.45    Entropy: 4.77    VF Loss: 0.20    Execution time: 00:00:01\n",
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_10\\checkpoint_000007\n",
      "\n",
      "Blue Scores so far ['2.25', '2.32', '2.50', '2.54', '2.52', '2.50', '2.52', '2.49', '2.47', '2.43']\n",
      "Red Scores so far ['2.71', '2.85', '2.82', '2.65', '2.71', '2.65', '2.51', '2.49', '2.79', '2.78']\n",
      "Total time:  00:00:24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "\n",
    "# Select which generation we want to start training from\n",
    "# If we want to train from scratch set to 1\n",
    "# If we want to train from the latest generation, set to latest generation in the competitive policy pool\n",
    "start_from_generation = 1\n",
    "fresh = True\n",
    "if (start_from_generation != 1):\n",
    "    fresh = False\n",
    "\n",
    "# Total number of generations create for red agent\n",
    "generations = 10\n",
    "\n",
    "# Error checking: Update either the 'starting_from_generation' or 'total_generations' parameter\n",
    "if(start_from_generation >= generations):\n",
    "    raise ValueError(\"Starting generation and Total Generation incompatible\")\n",
    "\n",
    "# Number of batches without improvement before ending training\n",
    "tolerance = 3\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(fresh=fresh, opponent=True)\n",
    "print(\"Pass 1\")\n",
    "red = build_red_agent(fresh=fresh)\n",
    "print(\"Pass 2\")\n",
    "\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(start_from_generation, generations+1):\n",
    "\n",
    "    # Time how long each generation takes\n",
    "    g_time = time.time()\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    red.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = red.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "        vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = blue.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "        vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path) \n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print(\"Total time: \",time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_time)))\n",
    "    print()\n",
    "    \n",
    "    # print(f'-------- Sample Game for Generation {g} --------')\n",
    "    # sample(red, blue, verbose=True, show_policy=True)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Agents...\n",
      "Red Agent 10 expects a minimum of 1.00 points, against Blue Opponent 10.\tExecution time: 00:00:07\n",
      "Red Agent 9 expects a minimum of 0.00 points, against Blue Opponent 6.\tExecution time: 00:00:08\n",
      "Red Agent 8 expects a minimum of 0.70 points, against Blue Opponent 9.\tExecution time: 00:00:08\n",
      "Red Agent 7 expects a minimum of 1.00 points, against Blue Opponent 1.\tExecution time: 00:00:08\n",
      "Red Agent 6 expects a minimum of 0.00 points, against Blue Opponent 4.\tExecution time: 00:00:07\n",
      "Red Agent 5 expects a minimum of 1.00 points, against Blue Opponent 9.\tExecution time: 00:00:07\n",
      "Red Agent 4 expects a minimum of 1.00 points, against Blue Opponent 8.\tExecution time: 00:00:07\n"
     ]
    }
   ],
   "source": [
    "# Set the sample games (default is 50)\n",
    "sample_games = 1\n",
    "\n",
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "\n",
    "g_start = time.time()\n",
    "\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "\n",
    "    # If the generation exists\n",
    "    if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\")):\n",
    "        start = time.time()\n",
    "        path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "        red_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        red.restore(red_restore_path)\n",
    "    \n",
    "        # iterate through blue opponents\n",
    "        for b in range(generations,0,-1):\n",
    "\n",
    "            # If the generation exists\n",
    "            if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\")):\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "                blue_restore_path = path_file.read()\n",
    "                path_file.close()\n",
    "                blue.restore(blue_restore_path)\n",
    "                score = sample(red, blue, games=sample_games)\n",
    "                if score < r_min[r]:\n",
    "                    r_min[r] = score\n",
    "                    r_min_op[r] = b\n",
    "                    \n",
    "        print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.', end=\"\\t\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start)))\n",
    "        if r_min[r] > r_best_score:\n",
    "            r_best_score = r_min[r]\n",
    "            r_best_id = r\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "print(\"Total Execution Time: \", time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_start)))\n",
    "\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf, 2.0, 2.0, 1.0, 0.0, 1.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n"
     ]
    }
   ],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Scores During Training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m id_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mid\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(red_scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m data_plot \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mid_plot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mred_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBlue Scores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mblue_scores\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Score\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdata_plot, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed Agent Training Score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
