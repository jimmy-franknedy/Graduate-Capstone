{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import build_blue_agent, build_red_agent, sample, get_timesteps, get_algorithm_select\n",
    "\n",
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {},
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting blue dqn config\n",
      "./policies/dqn/30/blue_opponent_pool/opponent_blue_0\\checkpoint_000000\n",
      "Pass 1\n",
      "selecting red dqn config\n",
      "./policies/dqn/30/red_competitive_pool/competitive_red_0\\checkpoint_000000\n",
      "Pass 2\n",
      "\n",
      "+--------------------------------+\n",
      "| Red Competitive Training Start |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "| Generation 1 |\n",
      "+--------------+\n",
      "\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'num_recreated_workers': 0, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'mean_q': -0.0039722687, 'min_q': -0.006660239, 'max_q': -0.001342431, 'mean_td_error': -0.006663899, 'model': {}}, 'td_error': array([-0.0101444 , -0.00988729, -0.00449896, -0.00561369, -0.00573631,\n",
      "       -0.00422694, -0.0066497 , -0.00233723, -0.0068239 , -0.01072058],\n",
      "      dtype=float32), 'custom_metrics': {}, 'num_agent_steps_trained': 10.0}}, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 10, 'num_agent_steps_sampled': 1000, 'num_agent_steps_trained': 10, 'last_target_update_ts': 1000, 'num_target_updates': 1}, 'sampler_results': {'episode_reward_max': 6.200000000000001, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.896875, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 32, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.511627543020059, 'mean_inference_ms': 2.810130556266146, 'mean_action_processing_ms': 0.2008847506397749, 'mean_env_wait_ms': 11.714396486244354, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0}, 'episode_reward_max': 6.200000000000001, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.896875, 'episode_len_mean': 30.0, 'episodes_this_iter': 32, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.511627543020059, 'mean_inference_ms': 2.810130556266146, 'mean_action_processing_ms': 0.2008847506397749, 'mean_env_wait_ms': 11.714396486244354, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'num_agent_steps_sampled': 1000, 'num_agent_steps_trained': 10, 'num_env_steps_sampled': 1000, 'num_env_steps_trained': 10, 'num_env_steps_sampled_this_iter': 1000, 'num_env_steps_trained_this_iter': 10, 'timesteps_total': 1000, 'num_steps_trained_this_iter': 10, 'agent_timesteps_total': 1000, 'timers': {'training_iteration_time_ms': 617.553, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 425.581, 'learn_throughput': 23.497, 'synch_weights_time_ms': 9.242}, 'counters': {'num_env_steps_sampled': 1000, 'num_env_steps_trained': 10, 'num_agent_steps_sampled': 1000, 'num_agent_steps_trained': 10, 'last_target_update_ts': 1000, 'num_target_updates': 1}, 'done': False, 'episodes_total': 32, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': 'e3d0e23d19044d0f9c73cb783bfc44e3', 'date': '2024-04-17_14-49-43', 'timestamp': 1713390583, 'time_this_iter_s': 6.197700500488281, 'time_total_s': 6.197700500488281, 'pid': 5960, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': False, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000021A74558BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219EF1D3D00>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 6.197700500488281, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 18.672945261001587}\n",
      "Batch 1 -- Red Score: 2.90    Entropy: 0.00    VF Loss: 0.00    Execution time: 00:00:06\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'num_recreated_workers': 0, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'mean_q': 0.8389775, 'min_q': 0.69686186, 'max_q': 1.1333592, 'mean_td_error': 0.24130997, 'model': {}}, 'td_error': array([ 0.18007189,  0.16816396,  0.8685874 ,  0.04967397,  0.28637278,\n",
      "       -0.80942506,  0.7489583 ,  0.23742199,  0.19733477,  0.48593986],\n",
      "      dtype=float32), 'custom_metrics': {}, 'num_agent_steps_trained': 10.0}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 110, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 110, 'last_target_update_ts': 2000, 'num_target_updates': 3}, 'sampler_results': {'episode_reward_max': 6.200000000000001, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.7390625, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 32, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0, 4.0, 1.0, 4.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.3, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 2.0, 6.0, 1.0, 3.0, 2.0, 4.0, 1.3, 4.0, 2.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.494497962233502, 'mean_inference_ms': 2.6578760333088174, 'mean_action_processing_ms': 0.22258591468387795, 'mean_env_wait_ms': 11.303249830723798, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0}, 'episode_reward_max': 6.200000000000001, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.7390625, 'episode_len_mean': 30.0, 'episodes_this_iter': 32, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0, 4.0, 1.0, 4.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.3, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 2.0, 6.0, 1.0, 3.0, 2.0, 4.0, 1.3, 4.0, 2.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.494497962233502, 'mean_inference_ms': 2.6578760333088174, 'mean_action_processing_ms': 0.22258591468387795, 'mean_env_wait_ms': 11.303249830723798, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 110, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 110, 'num_env_steps_sampled_this_iter': 1000, 'num_env_steps_trained_this_iter': 100, 'timesteps_total': 2000, 'num_steps_trained_this_iter': 100, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 526.147, 'load_time_ms': 0.0, 'load_throughput': 0.0, 'learn_time_ms': 6.687, 'learn_throughput': 1495.461, 'synch_weights_time_ms': 8.732}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 110, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 110, 'last_target_update_ts': 2000, 'num_target_updates': 3}, 'done': False, 'episodes_total': 64, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': 'e3d0e23d19044d0f9c73cb783bfc44e3', 'date': '2024-04-17_14-49-48', 'timestamp': 1713390588, 'time_this_iter_s': 5.290756464004517, 'time_total_s': 11.488456964492798, 'pid': 5960, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': False, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000021A74558BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219EF1D3D00>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 11.488456964492798, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 18.672945261001587}\n",
      "Batch 2 -- Red Score: 2.74    Entropy: 0.00    VF Loss: 0.00    Execution time: 00:00:05\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'num_recreated_workers': 0, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'mean_q': 2.2780101, 'min_q': 1.9446876, 'max_q': 2.7533007, 'mean_td_error': 0.46266812, 'model': {}}, 'td_error': array([ 0.4007206 , -0.11860394,  0.03129697,  2.0539174 ,  0.19600677,\n",
      "        0.4265132 ,  0.34993458,  0.19508123,  0.7063513 ,  0.385463  ],\n",
      "      dtype=float32), 'custom_metrics': {}, 'num_agent_steps_trained': 10.0}}, 'num_env_steps_sampled': 3000, 'num_env_steps_trained': 210, 'num_agent_steps_sampled': 3000, 'num_agent_steps_trained': 210, 'last_target_update_ts': 3000, 'num_target_updates': 5}, 'sampler_results': {'episode_reward_max': 7.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.815, 'episode_len_mean': 30.0, 'episode_media': {}, 'episodes_this_iter': 36, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0, 4.0, 1.0, 4.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.3, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 2.0, 6.0, 1.0, 3.0, 2.0, 4.0, 1.3, 4.0, 2.0, 3.2, 6.0, 1.0, 2.0, 2.0, 4.0, 7.0, 2.0, 5.0, 2.0, 4.0, 3.0, 2.0, 4.0, 1.0, 2.0, 3.0, 5.0, 3.0, 4.0, 0.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 2.0, 5.0, 2.0, 2.0, 2.0, 4.0, 1.0, 3.0, 4.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.521329209577224, 'mean_inference_ms': 2.5568444271999904, 'mean_action_processing_ms': 0.22913629485514836, 'mean_env_wait_ms': 11.02207298115881, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0}, 'episode_reward_max': 7.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 2.815, 'episode_len_mean': 30.0, 'episodes_this_iter': 36, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [3.0, 1.0, 3.0, 1.0, 4.0, 4.0, 5.399999999999995, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.699999999999998, 3.0, 2.0, 3.4000000000000012, 6.200000000000001, 4.0, 2.0, 5.0, 1.0, 0.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0, 4.0, 1.0, 4.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.3, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 2.0, 6.0, 1.0, 3.0, 2.0, 4.0, 1.3, 4.0, 2.0, 3.2, 6.0, 1.0, 2.0, 2.0, 4.0, 7.0, 2.0, 5.0, 2.0, 4.0, 3.0, 2.0, 4.0, 1.0, 2.0, 3.0, 5.0, 3.0, 4.0, 0.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 2.0, 5.0, 2.0, 2.0, 2.0, 4.0, 1.0, 3.0, 4.0], 'episode_lengths': [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 4.521329209577224, 'mean_inference_ms': 2.5568444271999904, 'mean_action_processing_ms': 0.22913629485514836, 'mean_env_wait_ms': 11.02207298115881, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'num_healthy_workers': 4, 'num_agent_steps_sampled': 3000, 'num_agent_steps_trained': 210, 'num_env_steps_sampled': 3000, 'num_env_steps_trained': 210, 'num_env_steps_sampled_this_iter': 1000, 'num_env_steps_trained_this_iter': 100, 'timesteps_total': 3000, 'num_steps_trained_this_iter': 100, 'agent_timesteps_total': 3000, 'timers': {'training_iteration_time_ms': 528.338, 'load_time_ms': 0.507, 'load_throughput': 19717.488, 'learn_time_ms': 5.179, 'learn_throughput': 1930.786, 'synch_weights_time_ms': 9.552}, 'counters': {'num_env_steps_sampled': 3000, 'num_env_steps_trained': 210, 'num_agent_steps_sampled': 3000, 'num_agent_steps_trained': 210, 'last_target_update_ts': 3000, 'num_target_updates': 5}, 'done': False, 'episodes_total': 100, 'training_iteration': 3, 'trial_id': 'default', 'experiment_id': 'e3d0e23d19044d0f9c73cb783bfc44e3', 'date': '2024-04-17_14-49-53', 'timestamp': 1713390593, 'time_this_iter_s': 5.299029588699341, 'time_total_s': 16.78748655319214, 'pid': 5960, 'hostname': 'Jimmy-LP', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'environments.RedTrainer'>, 'env_config': {}, 'observation_space': MultiBinary(88), 'action_space': Discrete(38), 'env_task_fn': None, 'render_env': False, 'clip_rewards': False, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 4, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 25, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': True, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 10, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': False, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x0000021A74558BB0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': False, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'target_network_update_freq': 500, 'replay_buffer_config': {'type': <class 'ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer'>, 'prioritized_replay': -1, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'worker_side_prioritization': False, 'replay_mode': 'independent'}, 'store_buffer_in_checkpoints': False, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'before_learn_on_batch': None, 'training_intensity': None, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x00000219EF1D3D00>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf', 'num_cpus_for_driver': 1}, 'time_since_restore': 16.78748655319214, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'warmup_time': 18.672945261001587}\n",
      "Batch 3 -- Red Score: 2.81    Entropy: 0.00    VF Loss: 0.00    Execution time: 00:00:05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 67\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     69\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:349\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[0;32m    348\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 349\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:673\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m     (\n\u001b[0;32m    666\u001b[0m         results,\n\u001b[0;32m    667\u001b[0m         train_iter_ctx,\n\u001b[0;32m    668\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2582\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[0;32m   2581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 2582\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2584\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\dqn\\dqn.py:362\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m store_weight, sample_and_train_weight \u001b[38;5;241m=\u001b[39m calculate_rr_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(store_weight):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# Sample (MultiAgentBatch) from workers.\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     new_sample_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# Update counters\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_sample_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:100\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[0;32m     97\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\worker.py:2274\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2271\u001b[0m     )\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2274\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\worker.py:669\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    668\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 669\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "\n",
    "# Select which generation we want to start training from\n",
    "# If we want to train from scratch set to 1 (default value is 1)\n",
    "# If we want to train from the latest generation, set to latest generation in the competitive policy pool\n",
    "start_from_generation = 1\n",
    "\n",
    "# Load rewards if training from a checkpointed generation\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "# Total number of generations create for red agent\n",
    "generations = 25\n",
    "\n",
    "# Error checking: Update either the 'starting_from_generation' or 'total_generations' parameter\n",
    "if(start_from_generation > generations):\n",
    "    raise ValueError(\"Starting generation and Total Generation incompatible\")\n",
    "\n",
    "# Number of batches without improvement before ending training\n",
    "tolerance = 3\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(fresh=True, opponent=True)\n",
    "print(\"Pass 1\")\n",
    "red = build_red_agent(fresh=True)\n",
    "print(\"Pass 2\")\n",
    "\n",
    "# Convert float string into a float\n",
    "# Useful if loading rewards from certain generation\n",
    "blue_scores = [float(item) for item in blue_scores]\n",
    "red_scores = [float(item) for item in red_scores]\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(start_from_generation, generations+1):\n",
    "\n",
    "    # Time how long each generation takes\n",
    "    g_time = time.time()\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    red.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = red.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        # Algorithm specific score retrieval\n",
    "        red_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            red_score = result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = blue.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        \n",
    "        # Score retrieval based on algorithm\n",
    "        blue_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            blue_score = -result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print(\"Total time: \",time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_time)))\n",
    "    print()\n",
    "    \n",
    "    # print(f'-------- Sample Game for Generation {g} --------')\n",
    "    # sample(red, blue, verbose=True, show_policy=True)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample games (default is 50)\n",
    "sample_games = 1\n",
    "\n",
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "g_start = time.time()\n",
    "\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "\n",
    "    # If the generation exists\n",
    "    if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\")):\n",
    "        start = time.time()\n",
    "        path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "        red_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        red.restore(red_restore_path)\n",
    "    \n",
    "        # iterate through blue opponents\n",
    "        for b in range(generations,0,-1):\n",
    "\n",
    "            # If the generation exists\n",
    "            if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\")):\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "                blue_restore_path = path_file.read()\n",
    "                path_file.close()\n",
    "                blue.restore(blue_restore_path)\n",
    "                score = sample(red, blue, games=sample_games)\n",
    "                if score < r_min[r]:\n",
    "                    r_min[r] = score\n",
    "                    r_min_op[r] = b\n",
    "                    \n",
    "        print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.', end=\"\\t\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start)))\n",
    "        if r_min[r] > r_best_score:\n",
    "            r_best_score = r_min[r]\n",
    "            r_best_id = r\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "print(\"Total Execution Time: \", time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_start)))\n",
    "\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb8796-bb25-4d45-b98a-980f22a65e79",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores (First 5 points dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45227c17-9bf1-449c-9375-6dc56941b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Training Score\":red_scores[5:], \"Blue Scores\":blue_scores[5:]})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Min Expected Score\":r_min[6:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Exploitability\":r_exp[5:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
