{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e66922-f02c-4d2b-acfa-788061d7b72d",
   "metadata": {},
   "source": [
    "#### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c390ce88-f98f-47b6-b079-e4a3c7840d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Upload the Github Zipfile\n",
    "# Unzip the Github Zipfile\n",
    "# Create python 3.8             conda create --name cse297 python=3.8.10\n",
    "# Activate the environment      source activate cse297\n",
    "# Within env install ipyk       pip install ipykernel\n",
    "# Register kernel w/ jupy       python -m ipykernel install --user --name=capstone\n",
    "# Change to package dir         cd CybORG-Competitive/CybORG/\n",
    "# Install packages within env   pip install -e .\n",
    "# Select 'capstone' kernel\n",
    "# Confirm python version is 3.8.10\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf0e0-c508-40be-99a2-8f0d7f8a274b",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ad6beb-c5db-4cb8-aa2e-d7f6d2b25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import build_blue_agent, build_red_agent, sample, get_timesteps, get_algorithm_select\n",
    "from environments import build_cardiff_agent, sample_against_cardiff\n",
    "\n",
    "import ray\n",
    "import os, sys, shutil, time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5189ab-336e-4eef-bc6f-35c92b2137fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the blue cardiff agent!\n",
      "returing...\n",
      "blu_cardiff is:  <cardiff.cage2.Agents.MainAgent.MainAgent object at 0x0000017CEE5BAAC0>\n",
      "selecting blue ppo config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:28: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m   import pkg_resources\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\pkg_resources\\__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\pkg_resources\\__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m   declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m Creating blue opponent\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m calling competitive wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m wrapper.py:  122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m calling get_opponent_config()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m opponent is selecting ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.1/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21760)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./policies/ppo/30/blue_opponent_pool/opponent_blue_0\\checkpoint_000000\n",
      "blu is:  PPO\n",
      "selecting red ppo config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:28: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m   import pkg_resources\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\pkg_resources\\__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\pkg_resources\\__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m   declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m Creating red trainer\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m wrapper.py:  122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m calling get_opponent_config()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m opponent is selecting ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m C:\\Users\\takys\\miniconda3\\envs\\cse297\\lib\\site-packages\\ray\\_private\\ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.1/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14644)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sample game...\n",
      "wrapper.py:  122\n",
      "g:  0\n",
      "MainAgent.py - get_action() - agent chose action:  51\n",
      "environments.py - red action is:  4\n",
      "red_step is:  DiscoverRemoteSystems 10.0.88.192/28\n",
      "blu_step is:  DecoyFemitter User2\n",
      "MainAgent.py - get_action() - agent chose action:  116\n",
      "environments.py - red action is:  15\n",
      "red_step is:  DiscoverNetworkServices 10.0.88.195\n",
      "blu_step is:  DecoyTomcat User2\n",
      "MainAgent.py - get_action() - agent chose action:  55\n",
      "environments.py - red action is:  12\n",
      "red_step is:  DiscoverNetworkServices 10.0.88.202\n",
      "blu_step is:  DecoyHarakaSMPT Enterprise0\n",
      "cardiff decided to load_meander()\n",
      "MainAgent.py - get_action() - agent chose action:  107\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  DecoyTomcat Enterprise0\n",
      "MainAgent.py - get_action() - agent chose action:  43\n",
      "environments.py - red action is:  26\n",
      "red_step is:  ExploitRemoteService 10.0.88.195\n",
      "blu_step is:  DecoyFemitter Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  44\n",
      "environments.py - red action is:  34\n",
      "red_step is:  PrivilegeEscalate User1\n",
      "blu_step is:  DecoyFemitter Enterprise2\n",
      "MainAgent.py - get_action() - agent chose action:  144\n",
      "environments.py - red action is:  6\n",
      "red_step is:  DiscoverNetworkServices 10.0.64.184\n",
      "blu_step is:  Restore User4\n",
      "MainAgent.py - get_action() - agent chose action:  141\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Restore User1\n",
      "MainAgent.py - get_action() - agent chose action:  134\n",
      "environments.py - red action is:  26\n",
      "red_step is:  ExploitRemoteService 10.0.88.195\n",
      "blu_step is:  Restore Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  130\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  DecoyVsftpd User3\n",
      "MainAgent.py - get_action() - agent chose action:  43\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  DecoyFemitter Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  144\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Restore User4\n",
      "MainAgent.py - get_action() - agent chose action:  4\n",
      "environments.py - red action is:  14\n",
      "red_step is:  DiscoverNetworkServices 10.0.88.201\n",
      "blu_step is:  Analyse Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  120\n",
      "environments.py - red action is:  25\n",
      "red_step is:  ExploitRemoteService 10.0.88.201\n",
      "blu_step is:  DecoyVsftpd Enterprise0\n",
      "MainAgent.py - get_action() - agent chose action:  29\n",
      "environments.py - red action is:  26\n",
      "red_step is:  ExploitRemoteService 10.0.88.195\n",
      "blu_step is:  DecoyApache Enterprise0\n",
      "MainAgent.py - get_action() - agent chose action:  11\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Analyse User1\n",
      "MainAgent.py - get_action() - agent chose action:  141\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Restore User1\n",
      "MainAgent.py - get_action() - agent chose action:  144\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  Restore User4\n",
      "MainAgent.py - get_action() - agent chose action:  61\n",
      "environments.py - red action is:  26\n",
      "red_step is:  ExploitRemoteService 10.0.88.195\n",
      "blu_step is:  DecoyHarakaSMPT Op_Server0\n",
      "MainAgent.py - get_action() - agent chose action:  141\n",
      "environments.py - red action is:  6\n",
      "red_step is:  DiscoverNetworkServices 10.0.64.184\n",
      "blu_step is:  Restore User1\n",
      "MainAgent.py - get_action() - agent chose action:  144\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  Restore User4\n",
      "MainAgent.py - get_action() - agent chose action:  35\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  DecoyApache Op_Server0\n",
      "MainAgent.py - get_action() - agent chose action:  141\n",
      "environments.py - red action is:  26\n",
      "red_step is:  ExploitRemoteService 10.0.88.195\n",
      "blu_step is:  Restore User1\n",
      "MainAgent.py - get_action() - agent chose action:  11\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  Analyse User1\n",
      "MainAgent.py - get_action() - agent chose action:  144\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Restore User4\n",
      "MainAgent.py - get_action() - agent chose action:  134\n",
      "environments.py - red action is:  7\n",
      "red_step is:  DiscoverNetworkServices 10.0.64.186\n",
      "blu_step is:  Restore Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  141\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  Restore User1\n",
      "MainAgent.py - get_action() - agent chose action:  134\n",
      "environments.py - red action is:  23\n",
      "red_step is:  ExploitRemoteService 10.0.88.202\n",
      "blu_step is:  Restore Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  43\n",
      "environments.py - red action is:  17\n",
      "red_step is:  ExploitRemoteService 10.0.64.184\n",
      "blu_step is:  DecoyFemitter Enterprise1\n",
      "MainAgent.py - get_action() - agent chose action:  134\n",
      "environments.py - red action is:  11\n",
      "red_step is:  DiscoverNetworkServices 10.0.170.120\n",
      "blu_step is:  Restore Enterprise1\n",
      "finished a game\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.699999999999996"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing code for building a cardiff blue agent ( to be moved down to the correct cell later below!)\n",
    "blue_cardiff = build_cardiff_agent()\n",
    "print(\"blu_cardiff is: \",blue_cardiff)\n",
    "blue = build_blue_agent(fresh=True, opponent=True)\n",
    "print(\"blu is: \", blue)\n",
    "\n",
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "red = build_red_agent(fresh=False)\n",
    "\n",
    "# Load the 'optimal' red agent policy\n",
    "red_optimal_filepath = None\n",
    "with open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"r\") as red_file:\n",
    "    red_optimal_filepath = red_file.read()\n",
    "red.restore(red_optimal_filepath)\n",
    "print(\"starting sample game...\")\n",
    "\n",
    "# Sample game\n",
    "sample_against_cardiff(red,blue_cardiff,games=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e256a1-c23b-44d6-b4a9-9d0fc58881ad",
   "metadata": {},
   "source": [
    "#### Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ed4e2d-e46a-401f-a4a5-4140f61fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e7f-c9a3-4c77-8241-f3fdc45789a8",
   "metadata": {},
   "source": [
    "#### Train Competitive Red Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d9e0a-9eca-4831-b133-fc504da23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the timestep and the algorithm selected for training\n",
    "selected_timestep = get_timesteps()\n",
    "selected_algorithm = get_algorithm_select()\n",
    "\n",
    "# Select which generation we want to start training from\n",
    "# If we want to train from scratch set to 1 (default value is 1)\n",
    "# If we want to train from the latest generation, set to latest generation in the competitive policy pool\n",
    "start_from_generation = 1\n",
    "\n",
    "# Load rewards if training from a checkpointed generation\n",
    "blue_scores = []\n",
    "red_scores = []\n",
    "\n",
    "# Total number of generations create for red agent\n",
    "generations = 25\n",
    "\n",
    "# Error checking: Update either the 'starting_from_generation' or 'total_generations' parameter\n",
    "if(start_from_generation > generations):\n",
    "    raise ValueError(\"Starting generation and Total Generation incompatible\")\n",
    "\n",
    "# Number of batches without improvement before ending training\n",
    "tolerance = 3\n",
    "\n",
    "# Create Initial Policies\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "blue = build_blue_agent(fresh=True, opponent=True)\n",
    "print(\"Pass 1\")\n",
    "red = build_red_agent(fresh=True)\n",
    "print(\"Pass 2\")\n",
    "\n",
    "# Convert float string into a float\n",
    "# Useful if loading rewards from certain generation\n",
    "blue_scores = [float(item) for item in blue_scores]\n",
    "red_scores = [float(item) for item in red_scores]\n",
    "\n",
    "print()\n",
    "print(\"+--------------------------------+\")\n",
    "print(\"| Red Competitive Training Start |\")\n",
    "print(\"+--------------------------------+\")\n",
    "print()\n",
    "\n",
    "for g in range(start_from_generation, generations+1):\n",
    "\n",
    "    # Time how long each generation takes\n",
    "    g_time = time.time()\n",
    "\n",
    "    if (g < 10):\n",
    "        dashes = 14\n",
    "    elif (g < 100):\n",
    "        dashes = 15\n",
    "    else:\n",
    "        dashes = 16\n",
    "    print('+'+'-'*dashes+'+')            \n",
    "    print(f\"| Generation {g} |\")\n",
    "    print('+'+'-'*dashes+'+')\n",
    "    print()\n",
    "\n",
    "    red.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0\n",
    "    red_max = 0\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = red.train()\n",
    "        # print(result)\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        # Algorithm specific score retrieval\n",
    "        red_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            red_score = result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            red_score = result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Red Score: {red_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (red_score > red_max):\n",
    "                red_max = red_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = red.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "             # when agent is no longer improving, break and save the new best-response agent\n",
    "            else:\n",
    "                red_scores.append(red_max)\n",
    "                red.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_size = g\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    blue.restore(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_0/checkpoint_000000\")\n",
    "\n",
    "    b = 0 # b tracks the batches of training completed\n",
    "    blue_min = float('inf')\n",
    "    tol = tolerance\n",
    "    b_time = time.time()\n",
    "    while True:\n",
    "        b += 1\n",
    "        start = time.time()\n",
    "        result = blue.train()\n",
    "        end = time.time()\n",
    "        elapsed_time = end-start\n",
    "        \n",
    "        # Score retrieval based on algorithm\n",
    "        blue_score = None\n",
    "        if(selected_algorithm == \"ppo\" or selected_algorithm == \"dqn\"):\n",
    "            blue_score = -result[\"sampler_results\"][\"episode_reward_mean\"]\n",
    "        else:\n",
    "            blue_score = -result[\"episode_reward_mean\"]\n",
    "        entropy = vf_loss = 0\n",
    "        if(selected_algorithm != \"dqn\"):\n",
    "            entropy = result['info']['learner']['default_policy']['learner_stats']['entropy']\n",
    "            vf_loss = result['info']['learner']['default_policy']['learner_stats']['vf_loss']\n",
    "        print(f\"Batch {b} -- Blue Score: {blue_score:0.2f}    Entropy: {entropy:0.2f}    VF Loss: {vf_loss:0.2f}\", end=\"    \")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        if b > 1:\n",
    "            if (blue_score < blue_min):\n",
    "                blue_min = blue_score\n",
    "                tol = tolerance\n",
    "                checkpoint_path = blue.save(checkpoint_dir=f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}\")\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{g}/checkpoint_path\", \"w\")\n",
    "                path_file.write(checkpoint_path)\n",
    "                path_file.close()\n",
    "            elif(tol > 1):\n",
    "                tol -= 1\n",
    "            # when agent is no longer improving, break and save the new competitive agent\n",
    "            else:\n",
    "                blue_scores.append(blue_min)\n",
    "                blue.restore(checkpoint_path)\n",
    "                print('Batch time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-b_time)))\n",
    "                print(checkpoint_path)\n",
    "                break\n",
    "\n",
    "    pool_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/pool_size\", \"w\")\n",
    "    pool_file.write(str(pool_size))\n",
    "    pool_file.close()\n",
    "    print()\n",
    "\n",
    "    print(f'Blue Scores so far {[\"%.2f\" % i for i in blue_scores]}')\n",
    "    print(f'Red Scores so far {[\"%.2f\" % i for i in red_scores]}')\n",
    "    print(\"Total time: \",time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_time)))\n",
    "    print()\n",
    "    \n",
    "    # print(f'-------- Sample Game for Generation {g} --------')\n",
    "    # sample(red, blue, verbose=True, show_policy=True)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152adf67-db00-4d3a-9ee2-3faafbb690ec",
   "metadata": {},
   "source": [
    "#### Evaluate Red MinMax Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d441528-d9a3-4ca4-8c60-e0fa20ed999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample games (default is 50)\n",
    "sample_games = 1\n",
    "\n",
    "r_min = [float('inf')]*(generations+1) # red agent minimum scores\n",
    "r_min_op = [0]*(generations+1) # id of blue opponent that got max score\n",
    "r_best_score = 0\n",
    "r_best_id = 0\n",
    "\n",
    "# evaluate existing pool of agents\n",
    "print('Evaluating Agents...')\n",
    "g_start = time.time()\n",
    "\n",
    "# iteration through red agents\n",
    "for r in range(generations,0,-1):\n",
    "\n",
    "    # If the generation exists\n",
    "    if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\")):\n",
    "        start = time.time()\n",
    "        path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r}/checkpoint_path\", \"r\")\n",
    "        red_restore_path = path_file.read()\n",
    "        path_file.close()\n",
    "        red.restore(red_restore_path)\n",
    "    \n",
    "        # iterate through blue opponents\n",
    "        for b in range(generations,0,-1):\n",
    "\n",
    "            # If the generation exists\n",
    "            if(os.path.exists(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\")):\n",
    "                path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/blue_opponent_pool/opponent_blue_{b}/checkpoint_path\", \"r\")\n",
    "                blue_restore_path = path_file.read()\n",
    "                path_file.close()\n",
    "                blue.restore(blue_restore_path)\n",
    "                score = sample(red, blue, games=sample_games)\n",
    "                if score < r_min[r]:\n",
    "                    r_min[r] = score\n",
    "                    r_min_op[r] = b\n",
    "                    \n",
    "        print(f'Red Agent {r} expects a minimum of {r_min[r]:0.2f} points, against Blue Opponent {r_min_op[r]}.', end=\"\\t\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start)))\n",
    "        if r_min[r] > r_best_score:\n",
    "            r_best_score = r_min[r]\n",
    "            r_best_id = r\n",
    "print()\n",
    "print(f'Top performing Red Agent is generation {r_best_id}')\n",
    "print(\"Total Execution Time: \", time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-g_start)))\n",
    "\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/red_competitive_pool/competitive_red_{r_best_id}/checkpoint_path\", \"r\")\n",
    "red_competitive_path = path_file.read()\n",
    "path_file.close()\n",
    "path_file = open(f\"./policies/{selected_algorithm}/{selected_timestep}/competitive_red_policy\", \"w\")\n",
    "path_file.write(red_competitive_path)\n",
    "path_file.close()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcd59a-63ba-4c9b-8578-fe2da91d34a4",
   "metadata": {},
   "source": [
    "#### Observe Exploitability of each Red Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d4aa-bb24-4cbd-b55b-89b5f2ce3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_exp = [] # exploitability of each Red Policy\n",
    "for r in r_min[1:]:\n",
    "    r_exp.append(r_best_score-r)\n",
    "print(r_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833a004-747a-4d44-b4f8-c8987f49feaf",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac02176-d858-4a5f-befc-283e0aee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Training Score\":red_scores, \"Blue Scores\":blue_scores})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Min Expected Score\":r_min[1:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot, \"Exploitability\":r_exp})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"RedPolicy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb8796-bb25-4d45-b98a-980f22a65e79",
   "metadata": {},
   "source": [
    "#### Plot Red Training Scores (First 5 points dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45227c17-9bf1-449c-9375-6dc56941b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores During Training\n",
    "id_plot = [id for id in range(1,len(red_scores)+1)]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Training Score\":red_scores[5:], \"Blue Scores\":blue_scores[5:]})\n",
    "plt.figure()\n",
    "sns.lineplot(x = \"Generation\", y = \"Training Score\", data=data_plot, color='red', label=\"Red Agent Training Score\")\n",
    "sns.lineplot(x = \"Generation\", y = \"Blue Scores\", data=data_plot, color='blue', label=\"Blue Opponent Score\")\n",
    "plt.show()\n",
    "\n",
    "# Minmax Evaluation\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Min Expected Score\":r_min[6:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Min Expected Score\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Min Expected Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exploitability\n",
    "id_plot = [id for id in range(len(red_scores))]\n",
    "data_plot = pd.DataFrame({\"Generation\":id_plot[5:], \"Exploitability\":r_exp[5:]})\n",
    "plt.figure()\n",
    "sns.regplot(x = \"Generation\", y = \"Exploitability\", data=data_plot, color='red', scatter_kws={'s':5}, label=\"Red Policy Exploitability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef761fd-1a08-4a7d-8f7e-ee4ad00b07ca",
   "metadata": {},
   "source": [
    "#### Cardiff Blue Agent vs Competitive Red PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ee342-a1b6-412c-96a6-d8600b532001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Agent - Restore the best checkpointed path\n",
    "# Blu Agent - Restore Cardiff blue agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5275b6a-7f76-4c31-ad51-debb5e3184fe",
   "metadata": {},
   "source": [
    "#### Cardiff Blue Agent & Dedicated Red PPO Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666bc0-21c3-4c41-995f-5f9cc4eb63f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
